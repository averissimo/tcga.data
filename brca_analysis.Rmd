---
title: "Multivariate analysis of BRCA data (TCGA)"
author: "Marta Belchior Lopes"
date: "3rd November, 2016"
output:
  html_document:
    number_sections: yes
    toc: yes
  output: null
  pdf_document:
    number_sections: yes"N"
    toc: yes
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, warning=FALSE)
```

# Description of the data set

The Cancer Genome Atlas Breast Invasive Carcinoma (TCGA-BRCA) data collection is part of a larger effort to build a research community focused on connecting cancer phenotypes to genotypes by providing clinical images matched to subjects from The Cancer Genome Atlas ([TCGA](https://cancergenome.nih.gov/)). 

The BRCA data is publicly available (https://cancergenome.nih.gov/) and is decomposed into two data sets:

I) the gene expression data, composed of 20501 variables for a total of 1205 individuals, 1093 with primary solid tumor and 112 with normal tissue;

II) the clinical data is composed of 18 variables obtained from the same individuals.

# Importing the data set

The BRCA data set can also be extracted through the TCGA2STAT R package (https://cran.r-project.org/web/packages/TCGA2STAT/index.html): 

```{r}
library(TCGA2STAT)
#brca <- getTCGA(disease="BRCA",data.type="RNASeq2",type="RPKM",clinical=TRUE)
load("marta_brca.RData")
```

The following links explain i) the individuals' barcode and ii) the sample type code:

i)  [Link to individuals' barcode from tcga](https://wiki.nci.nih.gov/display/TCGA/TCGA+barcode?desktop=true&macroName=unmigrated-inline-wiki-markup)

ii) [Link to sample type code from tcga](https://gdc.cancer.gov/resources-tcga-users/tcga-code-tables/sample-type-codes)

For this study sample type 1 (primary solid tumor) and 11 (solid tissue normal) were used, with the goal of discriminating between tumor and normal tissue samples.

Pattern to retrieve TCGA patient and sample type from extended barcode

```{r}
tcga.barcode.pattern <- '(TCGA-[A-Z0-9a-z]{2}-[a-zA-Z0-9]{4})-([0-9]{2}).+'
# patient barcode for each column in brca$dat
tissue.barcode <- gsub(tcga.barcode.pattern, '\\1', colnames(brca$dat))
```

Getting the sample type

```{r}
tissue.type  <- as.numeric(gsub(tcga.barcode.pattern,'\\2', colnames(brca$dat)))

tissue.tumor.solid.ix <- tissue.type == 1
tissue.tumor.metast.ix <- tissue.type == 6
tissue.tumor.unknown.ix <- tissue.type <=10 & tissue.type != 1 & tissue.type != 6 
tissue.normal.ix <- tissue.type >= 10 & tissue.type < 20
tissue.control.ix <- tissue.type > 20
# tissue: tumor
tissue.tumor.solid <- brca$dat[,tissue.tumor.solid.ix]
tissue.tumor.metast <- brca$dat[,tissue.tumor.metast.ix]
tissue.tumor.unkonw <- brca$dat[,tissue.tumor.unknown.ix]
# tissue normal = solid tissue normal
tissue.normal <- brca$dat[,tissue.normal.ix]
# tissue control (no cases in BRCA)
tissue.control <- brca$dat[,tissue.control.ix]
```

Size of data from each tissue

```{r}
dim(tissue.tumor.solid)
dim(tissue.tumor.metast)
dim(tissue.tumor.unkonw)
dim(tissue.normal)
dim(tissue.control)
```

Patients with solid tumor samples

```{r}
tissue.barcode.tumor.solid <- tissue.barcode[tissue.tumor.solid.ix]
```

Building the clinical matrix (discard patients without tumor solid samples)

```{r}
patient.no.tumor.solid <- rownames(brca$clinical)[!(rownames(brca$clinical) %in% tissue.barcode.tumor.solid)]

clinical.tumor.solid <- brca$clinical[(rownames(brca$clinical) %in% tissue.barcode[tissue.tumor.solid.ix]),]
dim(clinical.tumor.solid)

clinical.tumor.solid <- clinical.tumor.solid[tissue.barcode.tumor.solid,]

clinical.tumor.metast <- brca$clinical[(rownames(brca$clinical) %in% tissue.barcode[tissue.tumor.metast.ix]),]
dim(clinical.tumor.metast)
```

Building the genomic matrix; combining "tumor" and "normal" individuals into a single data set

```{r}
data <- rbind(t(tissue.tumor.solid),t(tissue.normal))

data.rownames.tcga.short <- gsub(tcga.barcode.pattern,'\\1',rownames(data))
length(rownames(clinical.tumor.solid))
clinical.tumor <- clinical.tumor.solid[data.rownames.tcga.short,]
# arranging the clinical data in the same order of data

data.Y <- rbind(matrix(1,ncol(tissue.tumor.solid),1),matrix(0,ncol(tissue.normal),1))
colnames(data.Y) <-"TN" # tumor (T) and normal tissue (N); 1 and 0, respectively
```

## Building training and test sets

One fifth of the individuals will be assigned to test samples, as follows:

```{r}
data.train <- data[-seq(1,nrow(data),by=5),]
data.rownames.train.tcga.short <- gsub(tcga.barcode.pattern,'\\1',rownames(data.train))
data.Y.train <- data.Y[-seq(1,nrow(data),by=5)]

data.test <- data[seq(1,nrow(data),by=5),]
data.rownames.test.tcga.short <- gsub(tcga.barcode.pattern,'\\1',rownames(data.test))
data.Y.test <- data.Y[seq(1,nrow(data),by=5)]
```

Two types of normalization will be performed:

1. as suggested by xxxx et. al (), with each individual observation at each variable being divided by the square root of the sum of the values of that variable for all individuals, multiplied by the square root of the sum of the variable values for that individual;

2. the so-called standard normal variate (from the chemometrics domain), which correspond to subtracting each individual by its mean and dividing by its standard deviation

```{r}
data.train.nrm <- data.train/(sqrt(matrix(apply(data.train,2,sum),nrow(data.train),ncol(data.train),byrow=TRUE))*sqrt(matrix(apply(data.train,1,sum),nrow(data.train),ncol(data.train),byrow=FALSE)))

data.test.nrm <- data.test/(sqrt(matrix(apply(data.train,2,sum),nrow(data.test),ncol(data.test),byrow=TRUE))*sqrt(matrix(apply(data.test,1,sum),nrow(data.test),ncol(data.test),byrow=FALSE)))

data.train.snv <-(data.train-matrix(apply(data.train,1,mean),nrow(data.train),ncol(data.train),byrow=FALSE))/matrix(apply(data.train,1,sd),nrow(data.train),ncol(data.train),byrow=FALSE)

data.test.snv <-(data.test-matrix(apply(data.test,1,mean),nrow(data.test),ncol(data.test),byrow=FALSE))/matrix(apply(data.test,1,sd),nrow(data.test),ncol(data.test),byrow=FALSE)
```

# Generalized linear model (Binomial) with variable selection

When trying to model a data set with 20501 variables, as those measured to create the breast carcinoma data set, it is expected that multicollinearity problems arise. It is therefore highly recommended that the dimensionality of such a data set is reduced, so that multicollinearity can be minimised.

Next, we will fit a logistic model after a variable selection step.

The 'glmnet' package will be used. This package ï¬ts lasso and elastic-net model paths for regression, logistic and multinomial regression using coordinate descent. The algorithm is extremely fast, and exploits sparsity in the input x matrix where it exists.

```{r}
library(glmnet)

fit.sparse.logit.cv.glmnet <- cv.glmnet(as.matrix(data.train.snv),as.factor(data.Y.train),family="binomial")
#print(fit.sparse.logit.cv.glmnet)

plot(fit.sparse.logit.cv.glmnet)

fit.sparse.logit.cv.glmnet.lambda <- fit.sparse.logit.cv.glmnet$lambda.min 
# the value of lambda that gives minimum mean cross-validation error

fit.sparse.logit.glmnet <- glmnet(as.matrix(data.train.snv), as.factor(data.Y.train), family="binomial", alpha = 0.7, nlambda = 1,lambda=fit.sparse.logit.cv.glmnet.lambda) # alpha=1 is the lasso penalty, and alpha=0 the ridge penalty

fit.sparse.logit.glmnet$df
# The number of nonzero coefficients for the value of lambda chosen

fit.sparse.logit.glmnet.var.selected <- colnames(data.train.snv[,which(coef(fit.sparse.logit.glmnet, s = 'lambda.min') != 0)[-1]])

fit.sparse.logit.glmnet.var.selected
# The variables corresponding to the nonzero coefficients

fit.sparse.logit.glmnet.var.selected.index <- which(coef(fit.sparse.logit.glmnet, s = 'lambda.min') != 0)[-1]
# Indexes of the variables selected

```

Predicting for a new data set

```{r}
pred.sparse.logit.glmnet <- predict(fit.sparse.logit.cv.glmnet,as.matrix(data.test.snv),s="lambda.min",type="class")
plot(pred.sparse.logit.glmnet,xlab="Individuals (test)",ylab="Predicted (class)")
# 1 false negative and 1 false positive

# identify(predict.sparse.logit.glmnet,labels=as.character(seq(1,nrow(data.test.snv),1)))
# identify the above individuals

which(pred.sparse.logit.glmnet != data.Y.test)
# false negative = sample # 53; false positive = sample # 227

# assessing the clinical data of these patients
rownames(data.test.snv[c(which(pred.sparse.logit.glmnet != data.Y.test)),])
clinical.false.glmnet <- clinical.tumor[data.rownames.test.tcga.short[c(which(pred.sparse.logit.glmnet != data.Y.test))],]

# going through the clinical variables to find what is different for individuals misclassified
plot.aux <- rep(NA,nrow(clinical.tumor))
plot.aux[c(1:52,54:length(data.rownames.test.tcga.short))] <- clinical.tumor[data.rownames.test.tcga.short[c(1:52,54:length(data.rownames.test.tcga.short))],16]
matplot(clinical.tumor[data.rownames.test.tcga.short[c(1:52,54:length(data.rownames.test.tcga.short))],16],type = "p",pch = 1,col="black",xlab="Individuals (test)",ylab="")

plot.aux <- rep(NA,nrow(clinical.tumor))
plot.aux[53] <- clinical.tumor[data.rownames.test.tcga.short[53],16]
matpoints(plot.aux,type = "p",pch = 1,col="red",xlab="",ylab="")



matplot(t(data.test[c(1:52,54:sum(data.Y.test)),which(coef(fit.sparse.logit.glmnet, s = 'lambda.min') != 0)[-1]]),type = "l", lty = 1, lwd = 1, pch = 1,col="black",xlab="Selected variables (index)",ylab="")
matpoints(t(data.test[c(219:241),which(coef(fit.sparse.logit.glmnet, s = 'lambda.min') != 0)[-1]]),type = "l", lty = 1, lwd = 1, pch = 1,col="gray70")
matpoints(t(data.test[c(53,227),which(coef(fit.sparse.logit.glmnet, s = 'lambda.min') != 0)[-1]]),type = "l", lty = 1, lwd = 1, pch = 1,col=c("red","blue"))

identify(data.test[53,which(coef(fit.sparse.logit.glmnet, s = 'lambda.min') != 0)[-1]],labels=fit.sparse.logit.glmnet.var.selected,col="red")
# identifying the variables for which sample # 53 (predicted false negative) is near the values of the neighbour normal samples' ((i.e., ACss3, CD302, ESYT2, OLFM4, STAT5B))

identify(data.test[227,which(coef(fit.sparse.logit.glmnet, s = 'lambda.min') != 0)[-1]],labels=fit.sparse.logit.glmnet.var.selected,col="blue")
# identifying the variables for which sample # 227 (predicted false negative) is near the values of the neighbour tumor samples' (i.e., FOXA 1)

```

# Partial Least Squares - Discriminant Analysis (PLS-DA) 

This section introduces feature extraction, which aims at creating new variables fom the original ones, accounting for as much information as possible. 

Partial least squares (PLS) regression will be used for that purpose. PLS extracts k latent variables (i.e., linear combinations of the original varibles) that minimises the covariance between the explanatory variables and the response variable. PLS-DA will be evaluated independently, as well as combined with variable selection.

## Without variable selection

```{r}
library(caret)

fit.plsda <- plsda(as.matrix(data.train.snv),as.factor(data.Y.train), ncomp = 18, probMethod = "Softmax") # The softmax function transforms the model predictions to "probability-like" values (e.g. on [0, 1] and sum to 1)

print(fit.plsda)

cumsum(fit.plsda$Xvar/fit.plsda$Xtotvar) # variance explained in X for each extracted latent variable (60 % for 18 latent variables extracted)
```
The scores' plot

```{r}
plot(fit.plsda$scores[1:874,1],fit.plsda$scores[1:874,2],col="gray60",xlim=c(-90,55),ylim=c(-90,60),ylab="LV2",xlab="LV1") # tumor samples
points(fit.plsda$scores[875:964,1],fit.plsda$scores[875:964,2],col="black") # normal samples
#identify(fit.plsda$scores[1:964,1],fit.plsda$scores[1:964,2], labels=as.character(seq(1,nrow(data.train.snv),1))) 
# identify individuals placed in the between-classe zone
```

The loadings' plot

```{r}
plot(fit.plsda$loadings[,1], type="l",col="black",xlim=c(0,20501),ylim=c(-0.5,0.6),xlab="Variables (index)",ylab="coefficients")
points(fit.plsda$loadings[,2], type="l",col="gray60",xlim=c(0,20501),ylim=c(-0.5,0.6))
legend(17000,0.56,c("LV1","LV2"),bty="n",lty=c(1,1),lwd=c(2.5,2.5),col=c("black","gray60")) 
# identify(fit.plsda$loadings[,1], labels=colnames(data.train.snv)) # identify variables
# identify(fit.plsda$loadings[,2], labels=colnames(data.train.snv)) # identify variables
```

The biplot

```{r}
biplot(fit.plsda, comps = 1:2,var.axes = TRUE,arrow.len = 0.1,col=c("black","gray40"))
```

Predicting for a new data set

```{r}
pred.plsda.train <- predict(fit.plsda, as.matrix(data.train.snv))
confusionMatrix(pred.plsda.train,as.factor(data.Y.train))
pred.plsda.test <- predict(fit.plsda, as.matrix(data.test.snv))
confusionMatrix(pred.plsda.test,as.factor(data.Y.test))
# 1 false negative

which(pred.plsda.test != data.Y.test)
# identify the false negative individual (sample # 53) - potential outlier

#plot(as.numeric(pred.plsda.test))
#identify(pred.plsda.test,labels=as.character(seq(1,nrow(data.test.snv),1)))
```

## With variable selection 

By sparse PLS-DA (sPLS-DA)

sPLS-DA using the 'spls' library requires the h parameter to be tuned, varying between 0 and 1 - the closer to 1 the smaller variable selection size, so that it matched the variable selection sizes with the other approaches. sPLSDA-LOG/LDA are performed in two steps: one step for variable selection with sPLS and one step for classification ([Chun and Keles, 2010](https://www.ncbi.nlm.nih.gov/pubmed/20107611)).

### Using the 'spls' package

```{r}
library(spls)
```

Using the LDA classifier

```{r}
fit.splsda.LDA <- splsda(as.matrix(data.train.snv),as.factor(data.Y.train), K =18, eta = 0.8,kappa=0.5,classifier="lda",scale.x=FALSE)

length(fit.splsda.LDA$spls.fit$A)

# print(fit.splsda.LDA) # 1560 variables selected out of 20501

select.var.splsda.LDA.spls <- fit.splsda.LDA$spls.fit$A # selected variables

select.coef.splsda.LDA.spls <- fit.splsda.LDA$W # coefficients of the selected variables
```

Predicting for a new data set

```{r}
pred.splsda.LDA.train <- predict(fit.splsda.LDA, as.matrix(data.train.snv))
confusionMatrix(pred.splsda.LDA.train,as.factor(data.Y.train))
pred.splsda.LDA.test <- predict(fit.splsda.LDA, as.matrix(data.test.snv))
confusionMatrix(pred.splsda.LDA.test,as.factor(data.Y.test))
# 1 false negative and 1 false positive predicted for the test set

which(pred.splsda.LDA.test != data.Y.test)
# identify the false negative individual (sample # 53) - potential outlier
```

Using the logistic classifier

```{r}
fit.splsda.LOGIT <- splsda(as.matrix(data.train.snv),as.factor(data.Y.train), K = 8, eta = .6,kappa=0.5,classifier="logistic",scale.x=FALSE)

length(fit.splsda.LOGIT$spls.fit$A)
       
# print(fit.splsda.LOGIT) # xx variables selected out of 20501

select.var.splsda.LOGIT.spls <- fit.splsda.LOGIT$A # selected variables

select.coef.splsda.LOGIT.spls <- fit.splsda.LOGIT$W # coefficients of the selected variables
```

Predicting for a new data set

```{r}
pred.splsda.LOGIT.train <- predict(fit.splsda.LOGIT, as.matrix(data.train.snv))
confusionMatrix(pred.splsda.LOGIT.train,as.factor(data.Y.train))
pred.splsda.LOGIT.test <- predict(fit.splsda.LOGIT, as.matrix(data.test.snv))
confusionMatrix(pred.splsda.LOGIT.test,as.factor(data.Y.test))
# 1 false negative and 1 false positive predicted for the test set

which(pred.splsda.LOGIT.train != data.Y.train)
# identify the false negative individual (sample # 79) as potential outlier

which(pred.splsda.LOGIT.test != data.Y.test)
# identify the false negative individual (sample # 53) and the false positive individual (sample # 227) as potential outliers


```

The SCORES' plot

E.g., for the LOGIT classifier

```{r}
plot(fit.splsda.LOGIT$T[1:874,1],fit.splsda.LOGIT$T[1:874,2],col="gray60",xlim=c(-0.11,0.07),ylim=c(-0.11,0.07),ylab="LV2",xlab="LV1")
points(fit.splsda.LOGIT$T[875:964,1],fit.splsda.LOGIT$T[875:964,2],col="black")

# identify(fit.splsda.LOGIT$T[1:964,1],fit.splsda.LOGIT$T[1:964,2], labels=as.character(seq(1,nrow(data.train.snv),1))) 
# identify individuals placed in the between-classe zone
```

<!-- The LOADINGS' plot -->

<!-- Latent variable 1 -->
<!-- ```{r} -->
<!-- plot(fit.plsda$loadings[fit.splsda.LOGIT$A,1], type="p",col="gray60",xlim=c(0,132),ylim=c(-0.5,0.6),ylab="coefficients",xlab="Variable index") -->
<!-- points(fit.splsda.LOGIT$W[,1], type="p",col="black") -->
<!-- # identify(fit.plsda$loadings[fit.splsda.LOGIT$A,1], labels=colnames(data.train.snv)) # identify variables -->
<!-- ``` -->

<!-- Latent variable 2 -->
<!-- ```{r} -->
<!-- plot(fit.plsda$loadings[fit.splsda.LOGIT$A,2], type="p",col="black",xlim=c(0,132),ylim=c(-0.5,0.6),ylab="coefficients",xlab="Variable index") -->
<!-- points(fit.splsda.LOGIT$W[,2], type="p",col="red") -->
<!-- # identify(fit.plsda$loadings[fit.splsda.LOGIT$A,2], labels=colnames(data.train.snv)) # identify variables -->
<!-- ``` -->

<!-- ### Using the 'mixOmics' package -->

<!-- As for the 'spls' library, an input on the number of PLS dimensions is required. The number of variables to select on each dimension is also required for sPLD-DA using the 'mixOmics'library. A LASSO penalization is used for variable selection purposes. -->

<!-- The prediction step in sPLS-DA is directly obtained from the by-products of the sPLS ([Cao et al., 2011](http://bmcbioinformatics.biomedcentral.com/articles/10.1186/1471-2105-12-253)). -->

<!-- ```{r} -->
<!-- library(mixOmics) -->

<!-- fit.splsda.logit.mixOmics <- splsda(as.matrix(data.train.snv),as.factor(data.Y.train),ncomp=6, mode="classic") -->

<!-- fit.splsda.logit.mixOmics$explained_variance$X -->
<!-- # variance explained by the PLS components -->

<!-- pred.splsda.logit.train.mixOmics <- predict(fit.splsda.logit.mixOmics,as.matrix(data.test.snv)) -->

<!-- #plot(pred.splsda.logit.mixOmics) -->
<!-- #table(pred.splsda.logit.mixOmics$vote$mahalanobis.dist[,2], breast.TCGA$data.test$subtype) -->

<!-- ``` -->

<!-- The scores's plot -->
<!-- ```{r} -->
<!-- col.stim <- c("darkblue", "purple", "green4","red3")  -->
<!-- plotIndiv(fit.splsda.mixOmics,c(1,2),ind.names=FALSE,group = as.factor(data.Y.train),legend = TRUE, style = 'graphics',title="sPLS-DA scores",legend.title="status", col.per.group = col.stim) -->
<!-- identify(fit.splsda.mixOmics$variates$X[,1],fit.splsda.mixOmics$variates$X[,2], labels=rownames(fit.splsda.mixOmics$variates$X)) # identify individuals -->
<!-- ``` -->
<!-- We can later have a look at this outlying individual based on its corresponding genomic and clinical data. -->

<!-- The loadings's plot -->

<!-- ```{r} -->
<!-- par(mfrow=c(6,1),mar=c(1,1,1,1)) -->
<!-- plot(abs(fit.splsda.mixOmics$loadings$X[,1]),type="l",main="sPLS-DA loadings") -->
<!-- plot(abs(fit.splsda.mixOmics$loadings$X[,2]),type="l") -->
<!-- plot(abs(fit.splsda.mixOmics$loadings$X[,3]),type="l") -->
<!-- plot(abs(fit.splsda.mixOmics$loadings$X[,4]),type="l") -->
<!-- plot(abs(fit.splsda.mixOmics$loadings$X[,5]),type="l") -->
<!-- plot(abs(fit.splsda.mixOmics$loadings$X[,6]),type="l",xlab="variables") -->


<!-- identify(fit.splsda.mixOmics$variates$X[,1],fit.splsda.mixOmics$variates$X[,2], labels=rownames(fit.splsda.mixOmics$variates$X)) # identify individuals -->
<!-- ``` -->

<!-- Selected variables in each latent variable -->

<!-- ```{r} -->
<!-- select.var.splsda.mixOmics.PC1 <- which(fit.splsda.mixOmics$loadings$X[,1] != 0) -->
<!-- select.var.splsda.mixOmics.PC2 <- which(fit.splsda.mixOmics$loadings$X[,2] != 0) -->
<!-- select.var.splsda.mixOmics.PC3 <- which(fit.splsda.mixOmics$loadings$X[,3] != 0) -->
<!-- select.var.splsda.mixOmics.PC4 <- which(fit.splsda.mixOmics$loadings$X[,4] != 0) -->
<!-- select.var.splsda.mixOmics.PC5 <- which(fit.splsda.mixOmics$loadings$X[,5] != 0) -->
<!-- select.var.splsda.mixOmics.PC6 <- which(fit.splsda.mixOmics$loadings$X[,6] != 0) -->
<!-- ``` -->

<!-- # Biological meaning of selected variables -->

<!-- The ultimate aim when performing variable selection is to investigate whether the selected varibles (e.g., genes) have a biological meaning. It may happen that similar performances yiled from different modelling approaches, even though they select different variables. The biological meaning of the selected genes can be obtained from the GeneGo software(https://galter.northwestern.edu/Guides-and-Tutorials/genego-pathways-software-guide) by Ashburner et al. (2000) (https://www.ncbi.nlm.nih.gov/pubmed/10802651) that outputs process networks, gene ontology processes as well as the list of diseases potentially linked with the selected genes. -->


# Outlier detection

An inspection on potential outliers will be approach next. Two new data sets will be used for building a logistic model, either based on the previous sparse-based variable selection (VS) or the combination of sparse-based variable selection and feature extraction by PLS-DA (VSFE), as previously shown.

## Generalized linear model (Binomial)

### With variable selection

```{r}
data.glm.out.data.VS <- cbind(data.Y.train,data.train.snv[,fit.sparse.logit.glmnet.var.selected.index])
brca.glm.out.VS <- glm(as.factor(data.Y.train)~.,family=binomial,data=as.data.frame(data.glm.out.data.VS))

# print(brca.glm.out.VS)
```

```{r}
pred.brca.glm.out.train.VS <- predict (brca.glm.out.VS,newdata=as.data.frame(data.train.snv[,fit.sparse.logit.glmnet.var.selected.index]),type="response")
pred.brca.glm.out.test.VS <- predict (brca.glm.out.VS,newdata=as.data.frame(data.test.snv[,fit.sparse.logit.glmnet.var.selected.index]),type="response")

plot(pred.brca.glm.out.train.VS,col="black",ylab="Predicted class",main="Train set",xlab="Individuals")
abline(a=0.5,b=0,col="gray60")
```


```{r}
# detecting outliers

library(binomTools)

brca.glm.out.res.VS <- Residuals(brca.glm.out.VS)

brca.glm.out.cook.dist.VS <- cooks.distance(brca.glm.out.VS) # Cook's Distance

plot(brca.glm.out.cook.dist.VS,pch="*",cex=2,main="Influential observations by the Cook's Distance",ylab="Cook's distance",xlab="Individuals")
abline(a=4*mean(brca.glm.out.cook.dist.VS,na.rm=T),b=0,col="red") # add cutoff line

# identify potential outliers
identify(brca.glm.out.cook.dist.VS, labels=as.character(seq(1,nrow(data.train.snv),1)))
text(x=1:length(brca.glm.out.cook.dist.VS)+1,y=brca.glm.out.cook.dist.VS,labels=ifelse(brca.glm.out.cook.dist.VS>4*mean(brca.glm.out.cook.dist.VS,na.rm=T),names(brca.glm.out.cook.dist.VS),""),col=2) # add labels
```

```{r}

#plot.aux <-rep(NA,ncol(data.train))
#plot.aux[fit.sparse.logit.glmnet.var.selected.index] <- fit.sparse.logit.glmnet.var.selected.index 

which(brca.glm.out.cook.dist.VS>4*mean(brca.glm.out.cook.dist.VS,na.rm=T))

matplot(t(data.train[c(1:874),fit.sparse.logit.glmnet.var.selected.index]),type = "p", pch = 1, col="black",xlab="Selected variables",ylab="")
matpoints(t(data.train[c(90,159,272,502,705,855,857),fit.sparse.logit.glmnet.var.selected.index]),type = "p", pch=1, col=c("cyan","green","red","yellow","blue","red","gray60"))

```

Bonferroni Outlier test

```{r}
library("car")
outlier.test(brca.glm.out.VS)
# individual # 907, the most extreme observation, is not an outlier

```

<!-- ### With feature extraction by PLS-DA -->

<!-- Building a logistic model based on a new data matrix composed of 18 extracted components, which are linear combinations of the original variables. -->

<!-- ```{r} -->
<!-- data.glm.out.data.FE <- cbind(data.Y.train,fit.plsda$scores) -->
<!-- brca.glm.out.FE <- glm(as.factor(data.Y.train)~.,family=binomial,data=as.data.frame(data.glm.out.data.FE)) -->

<!-- #summary(brca.glm.out.FE) -->
<!-- ``` -->

<!-- ```{r} -->
<!-- pred.brca.glm.out.train.FE <-predict(brca.glm.out.FE,newdata=as.data.frame(data.train.snv%*%fit.plsda$loadings),type="response") -->
<!-- pred.brca.glm.out.test.FE <- predict (brca.glm.out.FE,newdata=as.data.frame(data.test.snv%*%fit.plsda$loadings),type="response") -->

<!-- plot(pred.brca.glm.out.train.FE,col="black",ylab="Predicted class",main="Train set",xlab="Individuals") -->
<!-- ``` -->


<!-- ```{r} -->
<!-- # detecting outliers -->

<!-- brca.glm.out.res.FE <- Residuals(brca.glm.out.FE) -->

<!-- brca.glm.out.cook.dist.FE <- cooks.distance(brca.glm.out.VSFE) # Cook's Distance -->

<!-- plot(brca.glm.out.cook.dist.FE,pch="*",cex=2,main="Influential observations by the Cook's Distance",ylab="Cook's distance", xlab="Individuals") -->
<!-- abline(a=4*mean(brca.glm.out.cook.dist.FE,na.rm=T),b=0,col="red") # add cutoff line -->

<!-- identify(brca.glm.out.cook.dist.FE, labels=as.character(seq(1,nrow(data.train.snv),1))) -->
<!-- # potential outliers are individuals 79 (previously misclassified by sPLS-LOGIT), 524, 894, 896, 901, 907, 908, 946 and 952. -->

<!-- text(x=1:length(brca.glm.out.cook.dist.FE)+1,y=brca.glm.out.cook.dist.FE,labels=ifelse(brca.glm.out.cook.dist.FE>4*mean(brca.glm.out.cook.dist.FE,na.rm=T),names(brca.glm.out.cook.dist.FE),""),col=2) # add labels -->
<!-- ``` -->

<!-- ```{r} -->
<!-- #plot.aux <-rep(NA,nrow(data.train)) -->
<!-- #plot.aux[fit.splsda.LOGIT$spls.fit$A] <- fit.splsda.LOGIT$spls.fit$A  -->

<!-- which(brca.glm.out.cook.dist.FE>4*mean(brca.glm.out.cook.dist.FE,na.rm=T)) -->

<!-- # comparing potential 'tumor' outliers with other 'tumor' individuals   -->
<!-- matplot(t(data.train[c(1:874),fit.sparse.logit.glmnet.var.selected.index]),type = "p", pch = 1, col="black",xlab="Selected variables",ylab="") -->
<!-- matpoints(t(data.train[c(79,524),fit.sparse.logit.glmnet.var.selected.index]),type = "p", pch=1, col=c("red","blue")) -->

<!-- # comparing potential 'normal' outliers with other 'normal' individuals  -->
<!-- matplot(t(data.train[c(875:964),fit.sparse.logit.glmnet.var.selected.index]),type = "p", pch = 1, col="black",xlab="Selected variables",ylab="") -->
<!-- matpoints(t(data.train[c(894,896,901,907,908,946,952),fit.sparse.logit.glmnet.var.selected.index]),type = "p", pch=1, col=c("cyan","green","red","yellow","blue","red","gray60","red")) -->
<!-- ``` -->

<!-- ```{r} -->
<!-- library("car") -->
<!-- outlier.test(brca.glm.out.FE) -->

<!-- # removing outlier # 79 -->
<!-- brca.glm.out.FE <- glm(as.factor(data.Y.train)~.,family=binomial,data=as.data.frame(data.glm.out.data.FE[c(1:78,80:length(data.Y.train)),])) -->

<!-- # individual # 907 is not an outlier -->
<!-- ``` -->

### With variable selection and feature extraction by PLS-DA

Building a logistic model based on a new data matrix composed of 8 extracted components, which are linear combinations of the variables selected by sPLS-DA.

<!-- # detach("package:mixOmics", unload=TRUE) -->

<!-- ```{r} -->

<!-- library(caret) -->

<!-- fit.plsda.out <- plsda(as.matrix(data.train.snv[,fit.splsda.LDA$spls.fit$A]),as.factor(data.Y.train), ncomp = 18, probMethod = "Softmax") # or Bayes -->

<!-- pred.plsda.out <- predict(fit.plsda,as.matrix(data.test.snv[,fit.splsda.LDA$spls.fit$A])) -->

<!-- confusionMatrix(pred.plsda.out,as.factor(data.Y.test)) -->
<!-- ``` -->


```{r}
data.glm.out.data.VSFE <- cbind(data.Y.train,data.train.snv[,fit.splsda.LOGIT$spls.fit$A]%*%fit.splsda.LOGIT$W)
brca.glm.out.VSFE <- glm(as.factor(data.Y.train)~.,family=binomial,data=as.data.frame(data.glm.out.data.VSFE))

summary(brca.glm.out.VSFE)
```

```{r}
pred.brca.glm.out.train.VSFE <- predict (brca.glm.out.VSFE,newdata=as.data.frame(data.train.snv[,fit.splsda.LOGIT$spls.fit$A]%*%fit.splsda.LOGIT$W),type="response")
pred.brca.glm.out.test.VSFE <- predict (brca.glm.out.VSFE,newdata=as.data.frame(data.test.snv[,fit.splsda.LOGIT$spls.fit$A]%*%fit.splsda.LOGIT$W),type="response")

plot(pred.brca.glm.out.train.VSFE,col="black",ylab="Predicted class",main="Train set",xlab="Individuals")
```


```{r}
# detecting outliers

brca.glm.out.res.VSFE <- Residuals(brca.glm.out.VSFE)

brca.glm.out.cook.dist.VSFE <- cooks.distance(brca.glm.out.VSFE) # Cook's Distance

plot(brca.glm.out.cook.dist.VSFE,pch="*",cex=2,main="Influential observations by the Cook's Distance",ylab="Cook's distance", xlab="Individuals")
abline(a=4*mean(brca.glm.out.cook.dist.VSFE,na.rm=T),b=0,col="red") # add cutoff line

identify(brca.glm.out.cook.dist.VSFE, labels=as.character(seq(1,nrow(data.train.snv),1)))
# potential outliers are individuals 79 (previously misclassified by sPLS-LOGIT), 524, 894, 896, 901, 907, 908, 946 and 952.

text(x=1:length(brca.glm.out.cook.dist.VSFE)+1,y=brca.glm.out.cook.dist.VSFE,labels=ifelse(brca.glm.out.cook.dist.VSFE>4*mean(brca.glm.out.cook.dist.VSFE,na.rm=T),names(brca.glm.out.cook.dist.VSFE),""),col=2) # add labels
```

```{r}
#plot.aux <-rep(NA,nrow(data.train))
#plot.aux[fit.splsda.LOGIT$spls.fit$A] <- fit.splsda.LOGIT$spls.fit$A 

which(brca.glm.out.cook.dist.VSFE>4*mean(brca.glm.out.cook.dist.VSFE,na.rm=T))

# comparing potential 'tumor' outliers with other 'tumor' individuals  
matplot(t(data.train[c(1:874),fit.sparse.logit.glmnet.var.selected.index]),type = "p", pch = 1, col="black",xlab="Selected variables",ylab="")
matpoints(t(data.train[c(79,524),fit.sparse.logit.glmnet.var.selected.index]),type = "p", pch=1, col=c("red","blue"))

# comparing potential 'normal' outliers with other 'normal' individuals 
matplot(t(data.train[c(875:964),fit.sparse.logit.glmnet.var.selected.index]),type = "p", pch = 1, col="black",xlab="Selected variables",ylab="")
matpoints(t(data.train[c(894,896,901,907,908,946,952),fit.sparse.logit.glmnet.var.selected.index]),type = "p", pch=1, col=c("cyan","green","red","yellow","blue","red","gray60","red"))
```

Bonferroni Outlier test

```{r}
library("car")
outlier.test(brca.glm.out.VSFE)

# removing outlier # 79
brca.glm.out.VSFE <- glm(as.factor(data.Y.train)~.,family=binomial,data=as.data.frame(data.glm.out.data.VSFE[c(1:78,80:length(data.Y.train)),]))

# individual # 907 is not an outlier
```
